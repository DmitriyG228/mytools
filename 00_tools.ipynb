{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from food.paths import *\n",
    "import docker\n",
    "import logging\n",
    "import psutil\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_image_from_url(url,path = None):\n",
    "    if url: response = requests.get(url, stream=True)\n",
    "    if path: pass\n",
    "    return Image.open(response.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def kill(kill_procs     =['photos_resize','scrape','photos_save','0_app']):\n",
    "    kill_command       = lambda x : os.system(f'pkill -f {x}')\n",
    "    [kill_command(p) for p in kill_procs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_hash_folder(id):\n",
    "    return hashlib.sha1(f'{id}'.encode('utf-8')).hexdigest()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_selected_cities():\n",
    "        q = \"\"\"select l.id, c.country,c.city from listings l\n",
    "                join listings_cities c on (l.id = c.listing_id) \"\"\"\n",
    "        df = pd.read_sql(q,engine)\n",
    "        r = df.groupby('city')['id'].count().sort_values().to_frame().sort_values('id',ascending=False)\n",
    "        r= r.reset_index().merge(df.drop_duplicates('city')[['country','city']],on='city')\n",
    "        r = r[~r.country.isin(['Russia','Ukraine'])]\n",
    "        r.to_csv('selected_cities.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_location_dicts():\n",
    "    cities_df = pd.read_sql('select city, country from cities',engine)\n",
    "    countries = cities_df[['country']].drop_duplicates().sort_values('country').reset_index(drop=True)['country']\n",
    "    cities = cities_df.sort_values(['country','city']).drop_duplicates().reset_index(drop = True)\n",
    "    cities = cities['country'] + ', ' + cities['city']\n",
    "\n",
    "    city_replace_dict    = {value:key for key, value in cities.to_dict().items()}\n",
    "    country_replace_dict = {value:key for key, value in countries.to_dict().items()}\n",
    "    to_pickle((city_replace_dict,country_replace_dict),'location_dicts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_top_n_countries(n):\n",
    "    return pd.read_sql(\"\"\"select country, count(*)\n",
    "                from photos p\n",
    "                join listings l on        (l.id = p.listing_id)\n",
    "                join listings_cities c on (c.listing_id = l.id)\n",
    "                group by country\"\"\",engine).sort_values('count',ascending=False).head(n).sort_values('country').country.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def path_info(path):\n",
    "    path = path.ls() if path.is_dir() else path\n",
    "    files = pd.DataFrame([[f,\n",
    "                           f.name,\n",
    "                           pd.Timestamp(f.stat().st_ctime,unit='s'),\n",
    "                           f.stat().st_size]\n",
    "                           for f in path],\n",
    "                           columns=['path','name','time','size']) #if is a directory\n",
    "    files['size']=(files['size']/10**6)\n",
    "    return files.sort_values('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Union\n",
    "\n",
    "def rm_r(target: Union[Path, str], only_if_empty: bool = False):\n",
    "    \"\"\"\n",
    "    Delete a given directory and its subdirectories.\n",
    "\n",
    "    :param target: The directory to delete\n",
    "    :param only_if_empty: Raise RuntimeError if any file is found in the tree\n",
    "    \"\"\"\n",
    "    target = Path(target).expanduser()\n",
    "    if not  target.is_dir(): \n",
    "        print (target, ' is not a directory')\n",
    "        return\n",
    "    for p in sorted(target.glob('**/*'), reverse=True):\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        p.chmod(0o666)\n",
    "        if p.is_dir():\n",
    "            p.rmdir()\n",
    "        else:\n",
    "            if only_if_empty:\n",
    "                raise RuntimeError(f'{p.parent} is not empty!')\n",
    "            p.unlink()\n",
    "    target.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def search_notebooks(kword,path=None):\n",
    "    if not path: return os.popen(                 f\"grep --include='*.ipynb' --exclude-dir='.ipynb_checkpoints' -rliw . -e '{kword}'\").read().split('\\n')\n",
    "    if  path:    return os.popen(f\"cd; cd {path};   grep --include='*.ipynb' --exclude-dir='.ipynb_checkpoints' -rliw . -e '{kword}'\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def df_chunk_generator(df,chunksize,shuffle=False):\n",
    "    if shuffle: df=df.sample(frac=1)\n",
    "    start,stop = 0,chunksize\n",
    "    for i in range(start,len(df),chunksize):\n",
    "        chunk = df.iloc[start:stop]\n",
    "        start+=chunksize\n",
    "        stop +=chunksize\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./00_custom_pandas.ipynb',\n",
       " './00_qdrant.ipynb',\n",
       " './01_search.ipynb',\n",
       " './00_clipmodel.ipynb',\n",
       " './00_tools.ipynb',\n",
       " './01_multiple_foods.ipynb',\n",
       " './00_psql.ipynb',\n",
       " './archive/multiple_vector_math.ipynb',\n",
       " './00_paths.ipynb',\n",
       " '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_notebooks('#export')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def docker_container(name):\n",
    "    client = docker.from_env()\n",
    "    return [c for c in client.containers.list(all=True) if c.attrs['Name'] == f'/{name}'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LogDBHandler(logging.Handler):\n",
    "\n",
    "    def __init__(self,engine,sql_table,schema):\n",
    "        self.engine,self.sql_table,self.schema = engine,sql_table,schema\n",
    "        logging.Handler.__init__(self)\n",
    "\n",
    "    def emit(self, record):\n",
    "        df = pd.DataFrame(record.msg,index = [0])\n",
    "        df['level_no'] = record.levelname\n",
    "        df['timestamp'] = pd.Timestamp.utcnow()\n",
    "        df.to_sql(self.sql_table,self.engine,if_exists='append',index=False,schema = self.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_logger(engine,sql_table,schema,return_handler=False):\n",
    "    logger = logging.getLogger('main')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    bd_handler = LogDBHandler(engine,sql_table,schema)\n",
    "    logger.addHandler(bd_handler)\n",
    "    if return_handler: return logger,bd_handler\n",
    "    else:              return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_image_from_url(url=None):\n",
    "    response = requests.get(url, stream=True)\n",
    "    return Image.open(response.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compound_return(r,n): return ((1+r)**n)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import functools\n",
    "from types import FunctionType\n",
    "\n",
    "def copy_func(f):\n",
    "    \"Copy a non-builtin function (NB `copy.copy` does not work for this)\"\n",
    "    if not isinstance(f,FunctionType): return copy(f)\n",
    "    fn = FunctionType(f.__code__, f.__globals__, f.__name__, f.__defaults__, f.__closure__)\n",
    "    fn.__dict__.update(f.__dict__)\n",
    "    return fn\n",
    "def patch_to(cls, as_prop=False):\n",
    "    \"Decorator: add `f` to `cls`\"\n",
    "    if not isinstance(cls, (tuple,list)): cls=(cls,)\n",
    "    def _inner(f):\n",
    "        for c_ in cls:\n",
    "            nf = copy_func(f)\n",
    "            # `functools.update_wrapper` when passing patched function to `Pipeline`, so we do it manually\n",
    "            for o in functools.WRAPPER_ASSIGNMENTS: setattr(nf, o, getattr(f,o))\n",
    "            nf.__qualname__ = f\"{c_.__name__}.{f.__name__}\"\n",
    "            setattr(c_, f.__name__, property(nf) if as_prop else nf)\n",
    "        return f\n",
    "    return _inner\n",
    "\n",
    "def patch(f):\n",
    "    \"Decorator: add `f` to the first parameter's class (based on f's type annotations)\"\n",
    "    cls = next(iter(f.__annotations__.values()))\n",
    "    return patch_to(cls)(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_pickle (obj, file_name):\n",
    "    import pickle\n",
    "    file = open(file_name, 'wb')\n",
    "    # dump information to that file\n",
    "    pickle.dump(obj, file, protocol=4)\n",
    "    # close the file\n",
    "    file.close()\n",
    "\n",
    "def from_pickle (file_name):\n",
    "#     print(f'unpickling {file_name} ')\n",
    "    import pickle\n",
    "    file = open(file_name, 'rb')\n",
    "    # dump information to that file\n",
    "    return pickle.load(file)\n",
    "    # close the file\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def telegram(bot_message):\n",
    "    import requests\n",
    "#     proxies_list = get_proxies()\n",
    "\n",
    "#     proxies = {'https': proxies_list[0]}\n",
    "\n",
    "    bot_token = '918570679:AAHGf8qed65479rj35M3uQ9oVS4rxuD2xrs'\n",
    "    bot_chatID = '78882798'\n",
    "    send_text = 'https://api.telegram.org/bot' + bot_token + '/sendMessage?chat_id=' + bot_chatID + '&parse_mode=Markdown&text=' + bot_message\n",
    "\n",
    "    response = requests.get(send_text)#, proxies=proxies)\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pdrows(n):\n",
    "    pd.options.display.max_rows=n\n",
    "    pd.options.display.min_rows=n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pd_highlight(df,v):\n",
    "    def highlight(s,v):\n",
    "        match = s == v\n",
    "        return ['background-color: yellow' if v else '' for v in match]\n",
    "    return df.style.apply(highlight, args=(v,),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#launch in the background\n",
    "def inline(func,args=None):\n",
    "    import threading\n",
    "    if not args: thread = threading.Thread(target=func)\n",
    "    else:        thread = threading.Thread(target=func, args=args)\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_map(df, sample=10000, **kwargs):\n",
    "    import plotly.express as px\n",
    "    if    sample >len(df):pass\n",
    "    else: df =df.sample(sample)\n",
    "    if 'lon' not in df.columns: df['lon'],df['lat'] = df.geometry.x,df.geometry.y\n",
    "    px.set_mapbox_access_token(mapbox_access_token)\n",
    "    return px.scatter_mapbox(df, lat='lat', lon='lon', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def htop():\n",
    "    htop = pd.DataFrame([proc.as_dict() for proc in psutil.process_iter()])\n",
    "   # htop = htop[htop['username']=='dima']\n",
    "    htop['create_time'] = pd.to_datetime(htop['create_time'],unit='s')\n",
    "    htop = htop[htop['memory_percent']>0]\n",
    "    htop = htop[htop['name']=='python']\n",
    "    htop['kernel'] = htop['cmdline'].apply(lambda x:x[-1])\n",
    "    htop =htop.sort_values('create_time',ascending=False)\n",
    "    return htop[['pid','kernel','num_threads','memory_percent','create_time','open_files','cpu_percent','status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_proxies():\n",
    "    proxies = pd.read_csv('proxy.txt')\n",
    "    proxies.columns = ['prixies']\n",
    "    proxies = proxies['prixies'].tolist()\n",
    "    proxies = [line.strip().split(':') for line in open(\"proxy.txt\", \"r\").readlines()]\n",
    "    return [f'http://{proxy[2]}:{proxy[3]}@{proxy[0]}:{proxy[1]}' for proxy in proxies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def append_csv(name, **kwargs):\n",
    "    with open(name, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(kwargs.keys())\n",
    "        writer.writerow(kwargs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def repeat_df(df,times):\n",
    "    df_dtypes = df.dtypes\n",
    "    df_cols = df.columns\n",
    "    df = pd.DataFrame(np.repeat(df.values,times,axis=0))\n",
    "    df.columns = df_cols\n",
    "    df = df.astype(df_dtypes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_sql(df,table,chunksize=None):\n",
    "    if chunksize: df.to_sql(table,engine,method='multi',if_exists='append',chunksize=chunksize,index=False)\n",
    "    else: df.to_sql(table,engine,method='multi',if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def timestamp2int(timestamp = pd.Timestamp.now()):\n",
    "    timestamp = pd.Timestamp(timestamp)\n",
    "    return int(int(timestamp.to_numpy())/10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "pd_timestamp = lambda timestamp:pd.to_datetime(timestamp,unit='ms',utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def startEndTimestamp(length,shift = 0):\n",
    "    end = pd.Timestamp.utcnow() - pd.Timedelta(f'{shift} days')\n",
    "    return end - pd.Timedelta(f'{length} days'), end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def docker_container(name):\n",
    "    client = docker.from_env()\n",
    "    return [c for c in client.containers.list(all=True) if c.attrs['Name'] == f'/{name}'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pd_set_float(points): pd.options.display.float_format = ('{:.'+f'{points}'+'f}').format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def plot_multiple_y(*series):\n",
    "    subfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    fig = px.line(series[0])\n",
    "    data = fig.data\n",
    "\n",
    "    for s in series[1:]:\n",
    "        fig2 = px.scatter(s,\n",
    "                          color_discrete_map={\n",
    "                          \"rate\": \"green\",\n",
    "                          \"rate_mean\": \"orange\"})\n",
    "        fig2.update_traces(yaxis=\"y2\")\n",
    "        data = data + fig2.data\n",
    "\n",
    "    subfig.add_traces(data)\n",
    "    subfig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sql_head(table, limit=5):\n",
    "    return pd.read_sql(f'select * from {table} limit {limit}',engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_clickable(val):\n",
    "    # target _blank to open new window\n",
    "    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(val, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "selected_countries = ['Austria', \n",
    "      'Belgium',\n",
    "      'Bulgaria', \n",
    "      'Croatia', \n",
    "      'Cyprus', \n",
    "      'Czechia', \n",
    "      'Denmark', \n",
    "      'Estonia', \n",
    "      'Finland', \n",
    "      'France', \n",
    "      'Germany', \n",
    "      'Greece', \n",
    "      'Hungary', \n",
    "      'Ireland', \n",
    "      'Italy', \n",
    "      'Latvia', \n",
    "      'Lithuania', \n",
    "      'Luxembourg', \n",
    "      'Malta', \n",
    "      'Netherlands', \n",
    "      'Poland', \n",
    "      'Portugal', \n",
    "      'Romania', \n",
    "      'Slovakia', \n",
    "      'Slovenia', \n",
    "      'Spain',\n",
    "      'Sweden',\n",
    "      'Switzerland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_clipmodel.ipynb.\n",
      "Converted 00_custom_pandas.ipynb.\n",
      "Converted 00_paths.ipynb.\n",
      "Converted 00_progress_check.ipynb.\n",
      "Converted 00_psql.ipynb.\n",
      "Converted 00_qdrant.ipynb.\n",
      "Converted 00_tools.ipynb.\n",
      "Converted 01_search copy.ipynb.\n",
      "Converted 01_search.ipynb.\n",
      "Converted 02_onehot.ipynb.\n",
      "Converted colr_segmentation.ipynb.\n",
      "Converted food_clustering.ipynb.\n",
      "Converted inference.ipynb.\n",
      "Converted search_segmented.ipynb.\n",
      "Converted search_segmented_stable.ipynb.\n",
      "No export destination, ignored:\n",
      "#export\n",
      "def crop_image_to_square(img):\n",
      "    cropper = T.CenterCrop(min(img.size))\n",
      "    return cropper(img)\n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dima/anaconda3/envs/food/bin/nbdev_build_lib\", line 8, in <module>\n",
      "    sys.exit(nbdev_build_lib())\n",
      "  File \"/home/dima/anaconda3/envs/food/lib/python3.9/site-packages/fastcore/script.py\", line 113, in _f\n",
      "    tfunc(**merge(args, args_from_prog(func, xtra)))\n",
      "  File \"/home/dima/anaconda3/envs/food/lib/python3.9/site-packages/nbdev/export2html.py\", line 465, in nbdev_build_lib\n",
      "    notebook2script(fname=fname, bare=bare)\n",
      "  File \"/home/dima/anaconda3/envs/food/lib/python3.9/site-packages/nbdev/export.py\", line 441, in notebook2script\n",
      "    for f in sorted(files): d = _notebook2script(f, modules, silent=silent, to_dict=d, bare=bare)\n",
      "  File \"/home/dima/anaconda3/envs/food/lib/python3.9/site-packages/nbdev/export.py\", line 368, in _notebook2script\n",
      "    if to_dict is None: _add2all(fname_out, [f\"'{f}'\" for f in names if '.' not in f and len(f) > 0] + extra)\n",
      "  File \"/home/dima/anaconda3/envs/food/lib/python3.9/site-packages/nbdev/export.py\", line 208, in _add2all\n",
      "    with open(fname, 'r', encoding='utf8') as f: text = f.read()\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/dima/food/food/None.py'\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e1ee659a212ecca49d6a1809275141ef7642c152d6286fb803a43d965656456"
  },
  "kernelspec": {
   "display_name": "re",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

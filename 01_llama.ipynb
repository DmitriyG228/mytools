{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11248a2-bc27-4d07-b664-9ab41cf1fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3eacf8-2ea8-44f0-83b7-bb80befee8d1",
   "metadata": {},
   "source": [
    "##### this nb contains custom llama index classes asa module, as well as snippets to work with llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c7d3e-8ad4-445a-bb16-bb6f01ef8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe328ec-1cf4-4cff-a267-0b90bac67a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be1843-b666-4cb4-a800-a5028851501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from llama_index.readers.base import BaseReader\n",
    "from llama_index.schema import Document\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.response_synthesizers.base import BaseSynthesizer\n",
    "from llama_index.prompts import BasePromptTemplate\n",
    "from llama_index.indices.service_context import ServiceContext\n",
    "from typing import Any, List, Optional, Sequence\n",
    "from llama_index.types import RESPONSE_TEXT_TYPE\n",
    "\n",
    "from llama_index.schema import Node, NodeWithScore\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.response_synthesizers import ResponseMode, get_response_synthesizer,Accumulate\n",
    "\n",
    "from llama_index.response_synthesizers.base import BaseSynthesizer\n",
    "from llama_index.prompts.base import BasePromptTemplate\n",
    "from llama_index.types import RESPONSE_TEXT_TYPE\n",
    "\n",
    "import asyncio\n",
    "from typing import Any, List, Optional, Sequence\n",
    "\n",
    "from llama_index.async_utils import run_async_tasks\n",
    "from llama_index.indices.service_context import ServiceContext\n",
    "from llama_index.prompts import BasePromptTemplate\n",
    "from llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\n",
    "from llama_index.response_synthesizers.base import BaseSynthesizer\n",
    "from llama_index.types import RESPONSE_TEXT_TYPE\n",
    "\n",
    "from llama_index.prompts.base import PromptTemplate\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from typing import Any, List, Optional, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c4ff3-58de-44d3-9178-1b5a669215e6",
   "metadata": {},
   "source": [
    "## funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd072e-6ae4-40fb-88ed-28ffb7648717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_embedding(q):\n",
    "    return torch.Tensor(service_context.embed_model._get_query_embedding(q))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0de16-f187-4f72-b759-f7f550358f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cos(emb1,emb2):\n",
    "    return F.cosine_similarity(torch.Tensor(emb1), torch.Tensor(emb2), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48349af0-f255-47d4-8526-0d15ec75d4ce",
   "metadata": {},
   "source": [
    "## custom llama index classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c9df8-c562-499f-b31a-7a1a359d7a5d",
   "metadata": {},
   "source": [
    "#### pandas to docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b9287-5a2e-4036-87b8-88029e59f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PandasReader(BaseReader):\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        concat_rows: bool = True,\n",
    "        col_joiner: str = \", \",\n",
    "        row_joiner: str = \"\\n\",\n",
    "        pandas_config: dict = {},\n",
    "        **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._concat_rows = concat_rows\n",
    "        self._col_joiner = col_joiner\n",
    "        self._row_joiner = row_joiner\n",
    "        self._pandas_config = pandas_config\n",
    "\n",
    "    def load_data(\n",
    "        self, df, extra_info: Optional[Dict] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Parse file.\"\"\"\n",
    "        text_list = df.apply(\n",
    "            lambda row: (self._col_joiner).join(row.astype(str).tolist()), axis=1\n",
    "        ).tolist()\n",
    "\n",
    "        if self._concat_rows:\n",
    "            return [\n",
    "                Document(\n",
    "                    text=(self._row_joiner).join(text_list), metadata=extra_info or {}\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            return [\n",
    "                Document(text=text, metadata=extra_info or {}) for text in text_list\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a07bc9-75f2-4dd0-a14c-b76824529020",
   "metadata": {},
   "source": [
    "### custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a242c67b-9d1c-4a83-a5c0-bba044ad4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "prompt =  \"\"\"In order to {query_str}, please extract information from 'the input', please follow this schema:\n",
    "\n",
    "For every organization mentioned, collect its name, details, location, and any URLs associated with it.\n",
    "For every person mentioned, capture their name, position, education, location, activity, LinkedIn profile, and other related URLs.\n",
    "For every post or activity related to a person, collect its details and related URLs.\n",
    "If the location of an organization or person is mentioned, note it down.\n",
    "For every relationship, identify the type and the entities involved. This can be a person working at an organization, someone commenting on a post, or other types of relations.\n",
    "Use the structure provided to organize the data in a logical manner.\n",
    "\n",
    "GRAPH_SCHEMA:\n",
    "```{{\n",
    "    \"Organization\": [ [\"C001\", \"\", \"\", []] ], // [ID, Name, Details, URLs]\n",
    "    \"Person\": [ [\"P001\", \"\", \"\", []] ], // [ID, Name, Position, URLs]\n",
    "    \"Post\": [ [\"PP001\", \"\", []] ],   // [ID, Details, URLs]\n",
    "    \"Location\": [ [\"L001\", \"\"] ],    // [ID, Name]\n",
    "    \"Education\": [ [\"E001\", \"\"] ],   // [ID, Name]\n",
    "    \"Activity\": [ [\"A001\", \"\", []] ],  // [ID, Details, URLs]\n",
    "    \"Relationship\": [ [\"R001\", \"RELATIONSHIP_TYPE\", \"P001\", \"C001\", \"\", []] ] // [ID, Type, Start, End, Details, URLs]\n",
    "}}\n",
    "\n",
    "parse 'the input' below:\n",
    "{context_str}\n",
    "\n",
    "Return graph\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f55db8-3481-4630-8b4f-3e0ed4887fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from llama_index.prompts.base import PromptTemplate\n",
    "class CustomPrompt(PromptTemplate):\n",
    "    def __init__(self, template: str, **kwargs: Any):\n",
    "        super().__init__(template=template, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b44848-0cc2-40f9-b47b-d29282c0763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = CustomPrompt('summarize')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d59380-cf9d-4cbd-9fde-cf4ad9f85276",
   "metadata": {},
   "source": [
    "### synthesizers \n",
    "##### context Nodes  -> LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a62a0e0-7718-43cf-ba37-a4bf311b8e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#prompt specified as CustomPrompt\n",
    "class Accumulate2Graph(BaseSynthesizer):\n",
    "    \n",
    "    \"\"\"Accumulate responses from multiple text chunks and save them into files.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_qa_template: Optional[BasePromptTemplate] = None,\n",
    "        service_context: Optional[ServiceContext] = None,\n",
    "        save_dir: str = 'responses',\n",
    "        streaming: bool = False,\n",
    "        use_async: bool = False,\n",
    "        prompt:str = ''\n",
    "    ) -> None:\n",
    "        super().__init__(service_context=service_context, streaming=streaming)\n",
    "        self._text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n",
    "        self._use_async = use_async\n",
    "\n",
    "        self.save_dir = save_dir\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        self.custom_template = CustomPrompt(prompt)#prompt goes here\n",
    "\n",
    "    async def aget_response(\n",
    "        self,\n",
    "        query_str: str,\n",
    "        text_chunks: Sequence[str],\n",
    "        separator: str = \"\\n---------------------\\n\",\n",
    "        **response_kwargs: Any,\n",
    "    ) -> RESPONSE_TEXT_TYPE:\n",
    "        if self._streaming:\n",
    "            raise ValueError(\"Unable to stream in Accumulate response mode\")\n",
    "\n",
    "        async def process_chunk(query_str, text_chunk):\n",
    "            output = await self._give_responses(query_str, text_chunk, use_async=True)\n",
    "            save_response_to_file(query_str, output[0],self.save_dir)  # Assuming each chunk returns a list with a single item.\n",
    "            return output\n",
    "\n",
    "        tasks = [process_chunk(query_str, text_chunk) for text_chunk in text_chunks]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "        return f\"Responses for query {query_str} have been saved to the directory: {self.save_dir}\"\n",
    "\n",
    "    def get_response(\n",
    "        self,\n",
    "        query_str: str,\n",
    "        text_chunks: Sequence[str],\n",
    "        separator: str = \"\\n---------------------\\n\",\n",
    "        **response_kwargs: Any,\n",
    "    ) -> RESPONSE_TEXT_TYPE:\n",
    "        if self._streaming:\n",
    "            raise ValueError(\"Unable to stream in Accumulate response mode\")\n",
    "\n",
    "        for text_chunk in text_chunks:\n",
    "            output = self._give_responses(query_str, text_chunk)\n",
    "            if output:\n",
    "                save_response_to_file(query_str, output[0],self.save_dir)\n",
    "            if not output:\n",
    "                print(f\"No response for text_chunk: {text_chunk}\")\n",
    "            else:\n",
    "                save_response_to_file(query_str, output[0],self.save_dir)\n",
    "                \n",
    "\n",
    "        return f\"Responses for query {query_str} have been saved to the directory: {self.save_dir}\"\n",
    "\n",
    "    def _give_responses(\n",
    "        self, query_str: str, text_chunk: str, use_async: bool = False\n",
    "    ) -> List[Any]:\n",
    "        text_qa_template = self.custom_template.partial_format(query_str=query_str)\n",
    "        text_chunks = self._service_context.prompt_helper.repack(text_qa_template, [text_chunk])\n",
    "        predictor = self._service_context.llm_predictor.apredict if use_async else self._service_context.llm_predictor.predict\n",
    "        \n",
    "\n",
    "        return [predictor(text_qa_template, context_str=cur_text_chunk) for cur_text_chunk in text_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15625203-abc3-4656-92b0-00d5911a0cbc",
   "metadata": {},
   "source": [
    "response modes\n",
    "[responce modes](https://gpt-index.readthedocs.io/en/stable/core_modules/query_modules/query_engine/response_modes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eca2d7-50a6-486f-951c-4ccee7070d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomPrompt(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=[], kwargs={}, output_parser=None, template='summarize')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ec970-8241-434a-bbb3-93b0a29c23cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, got CustomPrompt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m service_context \u001b[38;5;241m=\u001b[39m ServiceContext\u001b[38;5;241m.\u001b[39mfrom_defaults()\n\u001b[0;32m----> 2\u001b[0m synthesizer \u001b[38;5;241m=\u001b[39m \u001b[43mAccumulate2Graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse contact details of a person\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m [NodeWithScore(node\u001b[38;5;241m=\u001b[39mNode(text\u001b[38;5;241m=\u001b[39mc), score\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chunks])\n",
      "Cell \u001b[0;32mIn[77], line 24\u001b[0m, in \u001b[0;36mAccumulate2Graph.__init__\u001b[0;34m(self, text_qa_template, service_context, save_dir, streaming, use_async, prompt)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(save_dir):\n\u001b[1;32m     23\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_template \u001b[38;5;241m=\u001b[39m \u001b[43mCustomPrompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[73], line 5\u001b[0m, in \u001b[0;36mCustomPrompt.__init__\u001b[0;34m(self, template, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, template: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/site-packages/llama_index/prompts/base.py:64\u001b[0m, in \u001b[0;36mPromptTemplate.__init__\u001b[0;34m(self, template, prompt_type, output_parser, metadata, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     62\u001b[0m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt_type\n\u001b[0;32m---> 64\u001b[0m template_vars \u001b[38;5;241m=\u001b[39m \u001b[43mget_template_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     67\u001b[0m     template\u001b[38;5;241m=\u001b[39mtemplate,\n\u001b[1;32m     68\u001b[0m     template_vars\u001b[38;5;241m=\u001b[39mtemplate_vars,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     output_parser\u001b[38;5;241m=\u001b[39moutput_parser,\n\u001b[1;32m     72\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/site-packages/llama_index/prompts/utils.py:12\u001b[0m, in \u001b[0;36mget_template_vars\u001b[0;34m(template_str)\u001b[0m\n\u001b[1;32m      9\u001b[0m variables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m formatter \u001b[38;5;241m=\u001b[39m Formatter()\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, variable_name, _, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate_str\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variable_name:\n\u001b[1;32m     14\u001b[0m         variables\u001b[38;5;241m.\u001b[39mappend(variable_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/string.py:288\u001b[0m, in \u001b[0;36mFormatter.parse\u001b[0;34m(self, format_string)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string):\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _string\u001b[38;5;241m.\u001b[39mformatter_parser(format_string)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, got CustomPrompt"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults()\n",
    "synthesizer = Accumulate2Graph(service_context=service_context,prompt=prompt)\n",
    "\n",
    "synthesizer.synthesize('parse contact details of a person',\n",
    "    nodes = [NodeWithScore(node=Node(text=c), score=1.0) for c in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dfaafe-6c46-42e0-88b5-49a52edf96a1",
   "metadata": {},
   "source": [
    "## snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c3cf73-8975-42ab-8daa-babc4a1dcf37",
   "metadata": {},
   "source": [
    "### openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171622a3-d839-44c7-b45d-e1c3eb2056a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import openai\n",
    "from mytools.tools import get_api_key\n",
    "from mytools.paths import *\n",
    "openai.api_key = get_api_key(api_path, 'openai')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32e7ea-1486-4da9-b6e3-db9daf8de388",
   "metadata": {},
   "source": [
    "### documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27636255-c1b5-4e10-af59-be966780b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from folder\n",
    "documents = SimpleDirectoryReader('data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87887913-1fe2-4e51-a5f4-9d20852c8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas \n",
    "df = pd.DataFrame()\n",
    "pdreader = PandasReader()\n",
    "documents = pdreader.load_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c16f53-5303-42b7-ab94-10146a2257ae",
   "metadata": {},
   "source": [
    "### index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d1ffd-6f53-45ba-ba84-deae6f739b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843be6df-2a8b-421c-a865-7682d324e6e4",
   "metadata": {},
   "source": [
    "### query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ede569-1e2f-4843-a0c7-419f10ccba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e330d1-f3b5-46b0-a945-476b549f537e",
   "metadata": {},
   "source": [
    "### chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1092e1-43fc-47b5-8cf4-42dfc3cc109c",
   "metadata": {},
   "source": [
    "#### memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be124533-bd36-4d09-9d1a-65d175ec4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple\n",
    "from llama_index.memory import ChatMemoryBuffer\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7ffd0-3f22-4cc6-9de5-272225227f74",
   "metadata": {},
   "source": [
    "#### chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6a7dc-749a-45b7-98ec-dfa179a7ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    system_prompt=\"You are a chatbot, able to have normal interactions, as well as talk about an essay discussing Paul Grahams life.\",\n",
    ")\n",
    "\n",
    "response = chat_engine.chat(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d28a5-8ef7-4cf7-ab80-f2cd71e19b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='Hello! How can I assist you today?', sources=[ToolOutput(content='system: Context information is below.\\n--------------------\\n\\n--------------------\\nYou are a chatbot, able to have normal interactions, as well as talk about an essay discussing Paul Grahams life.', tool_name='retriever', raw_input={'message': 'Hello!'}, raw_output=ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='Context information is below.\\n--------------------\\n\\n--------------------\\nYou are a chatbot, able to have normal interactions, as well as talk about an essay discussing Paul Grahams life.', additional_kwargs={}))], source_nodes=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1518e-1455-4ceb-83d3-fa4570f2d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f67fbdd-36c4-4e01-ac62-a2659a531e9f",
   "metadata": {},
   "source": [
    "### specify LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8114a9d5-a7a6-431e-9591-cf3d70205d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.1, model=\"gpt-4\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do after his time at Y Combinator?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# AUTOGENERATED! DO NOT EDIT! File to edit: ../00_chatgptfuncs.ipynb.

# %% auto 0
__all__ = ['processor', 'tokenizer', 'enc', 'extract_urls', 'get_button_text', 'prompt', 'ask_chatgpt', 'chat',
           'ask_chatgpt_stream', 'ask_chatgpt_stream_async', 'crop_tokens', 'strip_text']

# %% ../00_chatgptfuncs.ipynb 1
import re
from .paths import *
from .tools import *
import openai
openai.api_key = get_api_key(api_path,'openai')

# %% ../00_chatgptfuncs.ipynb 2
from transformers import CLIPProcessor, CLIPModel
processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
tokenizer = processor.tokenizer
import pandas as pd

# %% ../00_chatgptfuncs.ipynb 3
def prompt(role,text): return {"role": role, "content": text}

# %% ../00_chatgptfuncs.ipynb 4
def _ask_chatgpt(text,messages=[],prefix=[],role = 'user',return_request=False,model="gpt-3.5-turbo-0301",**kwargs):
    request = prompt(role,text)
    messages = prefix+messages+[request]
    
    return openai.ChatCompletion.create(
                                         model=model,  #"gpt-4", #"gpt-4" "gpt-3.5-turbo" gpt-3.5-turbo-0301
                                         messages=messages,
                                         **kwargs
                                        )

# %% ../00_chatgptfuncs.ipynb 7
def ask_chatgpt(text,messages=[],prefix=[],role = 'user',return_request=False,model="gpt-3.5-turbo-0301",**kwargs):
    request = prompt(role,text)
    r = _ask_chatgpt(text,messages=messages,prefix=prefix,role = 'user',return_request=False,model=model,**kwargs)
    response = r.choices[0].message
    print (response.content)
    if return_request: return response,request
    else: return response
    

# %% ../00_chatgptfuncs.ipynb 9
def chat(text, messages=[],prefix=[],model='gpt-3.5-turbo',append=True,role='user',**kwargs):
    response,request = ask_chatgpt(text,messages=messages,prefix=prefix,role=role,return_request=True,**kwargs)
    if append:
        messages.append(request)
        messages.append(response)
        
    return messages


# %% ../00_chatgptfuncs.ipynb 11
def ask_chatgpt_stream(text,messages=[],prefix=[],role = 'user',return_request=False,model="gpt-3.5-turbo-0301",**kwargs):
    
    r = _ask_chatgpt(text,messages=messages,prefix=prefix,role = 'user',return_request=False,model=model,stream = True,**kwargs)
    for i in r: 
        try:     yield i.choices[0].delta.content 
        except: pass

# %% ../00_chatgptfuncs.ipynb 13
async def ask_chatgpt_stream_async(text,messages=[],prefix=[],role = 'user',return_request=False,model="gpt-3.5-turbo-0301",**kwargs):
    
    r = _ask_chatgpt(text,messages=messages,prefix=prefix,role = 'user',return_request=False,model=model,stream = True,**kwargs)
    for i in r: 
        for _ in range(10):
            try:     
                yield i.choices[0].delta.content 
                break
            except: pass

# %% ../00_chatgptfuncs.ipynb 16
import tiktoken
enc = tiktoken.encoding_for_model("gpt-4")
def crop_tokens(messages,max_tokens=2500):
    token_count_df = pd.DataFrame(messages)
    token_count_df['len'] = token_count_df['content'].apply(lambda message: len(enc.encode(message)))
    token_count_df = token_count_df.sort_index(ascending=False)
    token_count_df['len']=token_count_df['len'].cumsum()
    token_count_df = token_count_df[token_count_df['len']<max_tokens]
    token_count_df = token_count_df.sort_index()
    token_index = token_count_df[token_count_df['len']<max_tokens].index
    return [messages[i] for i in token_index]

# %% ../00_chatgptfuncs.ipynb 19
def strip_text(text,urls):
    for url in urls: text = text.replace(url,'').strip()
    while True:
        start_len = len(text)
        text = text.replace('  ',' ')
        text = text.replace('\n \n','\n')
        stop_len = len(text)
        if start_len == stop_len: break
        
    return text
    

# %% ../00_chatgptfuncs.ipynb 20
extract_urls = lambda text: re.findall('https?://[^\s]+', text)

get_button_text = lambda url:url.split('q=')[1].split('&')[0].replace('+',' ')
